{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/Icd4mmLuWVTvvjnlnzXd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S0Q7gzjjKOLj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from typing import Tuple, Dict\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "UEXoP52I1CYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    # first conv block\n",
        "    self.conv1 = nn.Sequential(\n",
        "        # input channels (RGB image): 3\n",
        "        # output channels: 32 feature maps\n",
        "        # kernel size: 3x3\n",
        "        # padding: 1 to maintain spatial dimensions\n",
        "        # input size: 32x32x3\n",
        "        # output size: 32x32x32\n",
        "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "\n",
        "        # normalizes each of the 32 feature maps\n",
        "        # helps training by reducing internal covariate shift (see notes)\n",
        "        nn.BatchNorm2d(32),\n",
        "\n",
        "        # introduces non-linearity\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # takes 2x2 window and keeps maximum value\n",
        "        # stride of 2 means non-overlapping windows\n",
        "        # reduces spatial dimensions by half\n",
        "        # input size: 32x32x32\n",
        "        # output size: 16x16x32\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    # second conv block\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # output size: 64x8x8\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    # third conv block\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # output size: 128x4x4\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc = nn.Sequential(\n",
        "        # linear transformation\n",
        "        # This layer performs: output = weight_matrix(512 × 2048) × input(2048) + bias(512)\n",
        "        # 128 filters, 4x4 feature map from previous layer\n",
        "        # output size: 512 neurons\n",
        "        # see notes as to why 512 neurons was chosen\n",
        "        nn.Linear(128 * 4 * 4, 512),\n",
        "\n",
        "        # f(x) = max(0,x)\n",
        "        # see notes about ReLU\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # input: 512 features from previous layer\n",
        "        # output: 10 neurons (one for each CIFAR-10 class)\n",
        "        # This layer performs: output = weight_matrix(10 × 512) × input(512) + bias(10)\n",
        "        # the outputs represent the logits/score for each class\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "  # x = batch_size, 3 RGB channels, 32x32 image dimensions\n",
        "  # x.shape = (batch_size, 3, 32, 32)\n",
        "  def forward(self, x):\n",
        "    # Shape changes: (batch_size, 3, 32, 32) → (batch_size, 32, 16, 16)\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    # Shape changes: (batch_size, 32, 16, 16) → (batch_size, 64, 8, 8)\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    # Shape changes: (batch_size, 64, 8, 8) → (batch_size, 128, 4, 4)\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    # flattens 3D feature maps into 1D vector\n",
        "    # x.size(0) keeps the batch dimension\n",
        "    # -1: automatically calculates the flatten dimension (128*4*4 = 2048)\n",
        "    # Shape changes: (batch_size, 128, 4, 4) → (batch_size, 2048)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    # returns final logits\n",
        "    # will typically go into a loss function for training or softmax for predictions\n",
        "    return x"
      ],
      "metadata": {
        "id": "QnSL-662KdIc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "transform = transforms.Compose([\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.ToTensor(),\n",
        "\n",
        "  # see notes on how to compute these values\n",
        "  transforms.Normalize(\n",
        "    mean=[0.4914, 0.4822, 0.4465],\n",
        "    std=[0.2470, 0.2435, 0.2616]\n",
        "  )\n",
        "])"
      ],
      "metadata": {
        "id": "qb7Cw4S3UTYU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# split training data into train and validation sets (80/20 split)\n",
        "# use int() because we can't have fractional samples\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "gok3QJ5Ch_Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data loaders\n",
        "# larger batches = better GPU utilization, more stable gradients\n",
        "# shuffle=True: randomly reorders data each epoch\n",
        "# helps prevent the model from learning the order of data\n",
        "# num_workers=2: uses 2 parallel processes to load data. 2 is a conservative choice\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# no shuffling needed for validation because:\n",
        "# we don't train on this data.\n",
        "# consistent order helps with debugging\n",
        "# makes results reproducible\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "B0GH7EfMq0gc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "b7rXdGkurjQl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "  model.train() # sets model to training mode (affects batchnorm, dropout)\n",
        "  running_loss = 0.0 # accumulates loss over epoch\n",
        "  correct = 0 # counts correct predictions\n",
        "  total = 0 # counts total samples\n",
        "\n",
        "  for inputs, labels in train_loader: # iterates through batches\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad() # clears previous gradients\n",
        "    outputs = model(inputs) # forward pass: gets model predictions\n",
        "    loss = criterion(outputs, labels) # calculates loss\n",
        "    loss.backward() # computes gradients\n",
        "    optimizer.step() # updates model weights\n",
        "\n",
        "    running_loss += loss.item() # accumulates batch loss\n",
        "\n",
        "\n",
        "    _, predicted = outputs.max(1) # gets predicted class (highest probability)\n",
        "    total += labels.size(0) # adds batch size to total\n",
        "\n",
        "    # predicted.eq(labels) performs element-wise equality comparison between predicted and labels\n",
        "    # .sum() adds up all True values. returns single-element tensor with # of correct predictions\n",
        "    # .item() extracts scalar value from tensor\n",
        "    correct += predicted.eq(labels).sum().item() # counts correct predictions\n",
        "\n",
        "  # returns two metrics as a tuple\n",
        "  # average loss per batch for the epoch\n",
        "  # average correct percentage\n",
        "  return running_loss / len(train_loader), 100. * correct / total"
      ],
      "metadata": {
        "id": "TzntG7Cor9Vm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate function\n",
        "def validate(model, val_loader, criterion, device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return running_loss / len(val_loader), 100. * correct / total"
      ],
      "metadata": {
        "id": "cMS4f04htte0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model function\n",
        "def train_model(config: Dict = None):\n",
        "  # initialize wandb\n",
        "  if config is None:\n",
        "    config = {\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 128,\n",
        "        'num_epochs': 50,\n",
        "        'architecture': 'CIFAR10CNN',\n",
        "        'optimizer': 'Adam'\n",
        "    }\n",
        "\n",
        "  wandb.init(project='cifar10-cnn', config=config)\n",
        "  # log gradients and model parameters\n",
        "  wandb.watch(model)\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(config['num_epochs']):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # log metrics to wandb\n",
        "    wandb.log({\n",
        "        'epoch': epoch,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc\n",
        "    })\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}:')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
        "    print('-' * 50)\n",
        "\n",
        "  test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
        "  wandb.log({\n",
        "    'test_loss': test_loss,\n",
        "    'test_acc': test_acc\n",
        "  })\n",
        "\n",
        "  print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "-H0rv0Kw0YBd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jgeEjr0r3372",
        "outputId": "8181ea8d-9342-4ba4-a5ca-e03406983bb3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250220_202432-zuye7hmc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adampitchie-none/cifar10-cnn/runs/zuye7hmc' target=\"_blank\">flowing-frog-2</a></strong> to <a href='https://wandb.ai/adampitchie-none/cifar10-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adampitchie-none/cifar10-cnn' target=\"_blank\">https://wandb.ai/adampitchie-none/cifar10-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adampitchie-none/cifar10-cnn/runs/zuye7hmc' target=\"_blank\">https://wandb.ai/adampitchie-none/cifar10-cnn/runs/zuye7hmc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50:\n",
            "Train Loss: 1.4526 | Train Acc: 46.79%\n",
            "Val Loss: 1.2163 | Val Acc: 55.83%\n",
            "--------------------------------------------------\n",
            "Epoch 2/50:\n",
            "Train Loss: 0.9919 | Train Acc: 64.53%\n",
            "Val Loss: 1.0289 | Val Acc: 63.60%\n",
            "--------------------------------------------------\n",
            "Epoch 3/50:\n",
            "Train Loss: 0.8113 | Train Acc: 71.27%\n",
            "Val Loss: 0.8233 | Val Acc: 70.68%\n",
            "--------------------------------------------------\n",
            "Epoch 4/50:\n",
            "Train Loss: 0.7047 | Train Acc: 75.12%\n",
            "Val Loss: 0.8215 | Val Acc: 72.05%\n",
            "--------------------------------------------------\n",
            "Epoch 5/50:\n",
            "Train Loss: 0.6441 | Train Acc: 77.61%\n",
            "Val Loss: 0.7605 | Val Acc: 73.76%\n",
            "--------------------------------------------------\n",
            "Epoch 6/50:\n",
            "Train Loss: 0.5900 | Train Acc: 79.33%\n",
            "Val Loss: 0.7646 | Val Acc: 73.55%\n",
            "--------------------------------------------------\n",
            "Epoch 7/50:\n",
            "Train Loss: 0.5426 | Train Acc: 80.86%\n",
            "Val Loss: 0.6576 | Val Acc: 77.06%\n",
            "--------------------------------------------------\n",
            "Epoch 8/50:\n",
            "Train Loss: 0.5186 | Train Acc: 81.81%\n",
            "Val Loss: 0.6077 | Val Acc: 79.14%\n",
            "--------------------------------------------------\n",
            "Epoch 9/50:\n",
            "Train Loss: 0.4853 | Train Acc: 82.96%\n",
            "Val Loss: 0.6680 | Val Acc: 77.18%\n",
            "--------------------------------------------------\n",
            "Epoch 10/50:\n",
            "Train Loss: 0.4505 | Train Acc: 84.31%\n",
            "Val Loss: 0.6670 | Val Acc: 77.42%\n",
            "--------------------------------------------------\n",
            "Epoch 11/50:\n",
            "Train Loss: 0.4358 | Train Acc: 84.77%\n",
            "Val Loss: 0.5526 | Val Acc: 80.86%\n",
            "--------------------------------------------------\n",
            "Epoch 12/50:\n",
            "Train Loss: 0.4096 | Train Acc: 85.66%\n",
            "Val Loss: 0.5570 | Val Acc: 80.95%\n",
            "--------------------------------------------------\n",
            "Epoch 13/50:\n",
            "Train Loss: 0.3940 | Train Acc: 86.24%\n",
            "Val Loss: 0.5331 | Val Acc: 82.11%\n",
            "--------------------------------------------------\n",
            "Epoch 14/50:\n",
            "Train Loss: 0.3761 | Train Acc: 86.88%\n",
            "Val Loss: 0.5089 | Val Acc: 83.00%\n",
            "--------------------------------------------------\n",
            "Epoch 15/50:\n",
            "Train Loss: 0.3552 | Train Acc: 87.59%\n",
            "Val Loss: 0.5204 | Val Acc: 82.54%\n",
            "--------------------------------------------------\n",
            "Epoch 16/50:\n",
            "Train Loss: 0.3471 | Train Acc: 87.74%\n",
            "Val Loss: 0.4860 | Val Acc: 83.81%\n",
            "--------------------------------------------------\n",
            "Epoch 17/50:\n",
            "Train Loss: 0.3325 | Train Acc: 88.53%\n",
            "Val Loss: 0.5309 | Val Acc: 82.11%\n",
            "--------------------------------------------------\n",
            "Epoch 18/50:\n",
            "Train Loss: 0.3217 | Train Acc: 88.70%\n",
            "Val Loss: 0.5196 | Val Acc: 82.75%\n",
            "--------------------------------------------------\n",
            "Epoch 19/50:\n",
            "Train Loss: 0.3082 | Train Acc: 89.21%\n",
            "Val Loss: 0.4372 | Val Acc: 85.30%\n",
            "--------------------------------------------------\n",
            "Epoch 20/50:\n",
            "Train Loss: 0.2934 | Train Acc: 89.69%\n",
            "Val Loss: 0.4869 | Val Acc: 84.08%\n",
            "--------------------------------------------------\n",
            "Epoch 21/50:\n",
            "Train Loss: 0.2883 | Train Acc: 89.95%\n",
            "Val Loss: 0.4578 | Val Acc: 85.23%\n",
            "--------------------------------------------------\n",
            "Epoch 22/50:\n",
            "Train Loss: 0.2761 | Train Acc: 90.39%\n",
            "Val Loss: 0.4585 | Val Acc: 85.12%\n",
            "--------------------------------------------------\n",
            "Epoch 23/50:\n",
            "Train Loss: 0.2657 | Train Acc: 90.68%\n",
            "Val Loss: 0.4482 | Val Acc: 85.41%\n",
            "--------------------------------------------------\n",
            "Epoch 24/50:\n",
            "Train Loss: 0.2593 | Train Acc: 90.83%\n",
            "Val Loss: 0.4820 | Val Acc: 84.80%\n",
            "--------------------------------------------------\n",
            "Epoch 25/50:\n",
            "Train Loss: 0.2429 | Train Acc: 91.47%\n",
            "Val Loss: 0.4637 | Val Acc: 85.52%\n",
            "--------------------------------------------------\n",
            "Epoch 26/50:\n",
            "Train Loss: 0.2418 | Train Acc: 91.56%\n",
            "Val Loss: 0.4540 | Val Acc: 85.55%\n",
            "--------------------------------------------------\n",
            "Epoch 27/50:\n",
            "Train Loss: 0.2305 | Train Acc: 91.80%\n",
            "Val Loss: 0.4743 | Val Acc: 84.76%\n",
            "--------------------------------------------------\n",
            "Epoch 28/50:\n",
            "Train Loss: 0.2297 | Train Acc: 91.98%\n",
            "Val Loss: 0.4349 | Val Acc: 86.24%\n",
            "--------------------------------------------------\n",
            "Epoch 29/50:\n",
            "Train Loss: 0.2182 | Train Acc: 92.33%\n",
            "Val Loss: 0.4472 | Val Acc: 85.92%\n",
            "--------------------------------------------------\n",
            "Epoch 30/50:\n",
            "Train Loss: 0.2119 | Train Acc: 92.45%\n",
            "Val Loss: 0.4476 | Val Acc: 85.80%\n",
            "--------------------------------------------------\n",
            "Epoch 31/50:\n",
            "Train Loss: 0.2031 | Train Acc: 92.80%\n",
            "Val Loss: 0.4308 | Val Acc: 86.60%\n",
            "--------------------------------------------------\n",
            "Epoch 32/50:\n",
            "Train Loss: 0.1996 | Train Acc: 92.97%\n",
            "Val Loss: 0.5109 | Val Acc: 85.01%\n",
            "--------------------------------------------------\n",
            "Epoch 33/50:\n",
            "Train Loss: 0.1908 | Train Acc: 93.31%\n",
            "Val Loss: 0.4449 | Val Acc: 86.52%\n",
            "--------------------------------------------------\n",
            "Epoch 34/50:\n",
            "Train Loss: 0.1841 | Train Acc: 93.55%\n",
            "Val Loss: 0.4681 | Val Acc: 85.94%\n",
            "--------------------------------------------------\n",
            "Epoch 35/50:\n",
            "Train Loss: 0.1862 | Train Acc: 93.42%\n",
            "Val Loss: 0.4734 | Val Acc: 86.05%\n",
            "--------------------------------------------------\n",
            "Epoch 36/50:\n",
            "Train Loss: 0.1747 | Train Acc: 93.87%\n",
            "Val Loss: 0.5170 | Val Acc: 84.90%\n",
            "--------------------------------------------------\n",
            "Epoch 37/50:\n",
            "Train Loss: 0.1740 | Train Acc: 93.78%\n",
            "Val Loss: 0.4738 | Val Acc: 85.80%\n",
            "--------------------------------------------------\n",
            "Epoch 38/50:\n",
            "Train Loss: 0.1674 | Train Acc: 93.92%\n",
            "Val Loss: 0.4700 | Val Acc: 86.58%\n",
            "--------------------------------------------------\n",
            "Epoch 39/50:\n",
            "Train Loss: 0.1711 | Train Acc: 93.89%\n",
            "Val Loss: 0.4984 | Val Acc: 85.69%\n",
            "--------------------------------------------------\n",
            "Epoch 40/50:\n",
            "Train Loss: 0.1592 | Train Acc: 94.47%\n",
            "Val Loss: 0.4407 | Val Acc: 87.00%\n",
            "--------------------------------------------------\n",
            "Epoch 41/50:\n",
            "Train Loss: 0.1538 | Train Acc: 94.58%\n",
            "Val Loss: 0.5445 | Val Acc: 85.11%\n",
            "--------------------------------------------------\n",
            "Epoch 42/50:\n",
            "Train Loss: 0.1506 | Train Acc: 94.63%\n",
            "Val Loss: 0.5143 | Val Acc: 85.44%\n",
            "--------------------------------------------------\n",
            "Epoch 43/50:\n",
            "Train Loss: 0.1473 | Train Acc: 94.77%\n",
            "Val Loss: 0.4807 | Val Acc: 86.43%\n",
            "--------------------------------------------------\n",
            "Epoch 44/50:\n",
            "Train Loss: 0.1430 | Train Acc: 94.86%\n",
            "Val Loss: 0.4896 | Val Acc: 86.22%\n",
            "--------------------------------------------------\n",
            "Epoch 45/50:\n",
            "Train Loss: 0.1440 | Train Acc: 94.87%\n",
            "Val Loss: 0.4547 | Val Acc: 86.64%\n",
            "--------------------------------------------------\n",
            "Epoch 46/50:\n",
            "Train Loss: 0.1331 | Train Acc: 95.30%\n",
            "Val Loss: 0.4832 | Val Acc: 86.54%\n",
            "--------------------------------------------------\n",
            "Epoch 47/50:\n",
            "Train Loss: 0.1290 | Train Acc: 95.47%\n",
            "Val Loss: 0.5434 | Val Acc: 85.46%\n",
            "--------------------------------------------------\n",
            "Epoch 48/50:\n",
            "Train Loss: 0.1298 | Train Acc: 95.47%\n",
            "Val Loss: 0.4690 | Val Acc: 87.21%\n",
            "--------------------------------------------------\n",
            "Epoch 49/50:\n",
            "Train Loss: 0.1249 | Train Acc: 95.66%\n",
            "Val Loss: 0.5496 | Val Acc: 85.25%\n",
            "--------------------------------------------------\n",
            "Epoch 50/50:\n",
            "Train Loss: 0.1252 | Train Acc: 95.56%\n",
            "Val Loss: 0.4843 | Val Acc: 86.87%\n",
            "--------------------------------------------------\n",
            "Test Loss: 0.5234 | Test Acc: 86.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "FdSvWqaD4KNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}