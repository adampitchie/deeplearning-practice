{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiuy1Odbw2aTvpl9A178/c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QmjZtqKPbjtR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb login here\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "AkEfz9KPe-R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  # see notes on variable stride, shortcut/skip/residual connection, and bias-free convolutions\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = self.shortcut(x)\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "0bSoQdT2fGu0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  # see notes on variable stride, variable kernel_size, self.expansion, and ReLU\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(Bottleneck, self).__init__()\n",
        "\n",
        "    # focus only on channel dimensions, not spatial dimensions (like conv2)\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # variable stride. spatial reduction should happen alongside spatial feature extraction\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # focus only on channel dimensions, not spatial dimensions (like conv2)\n",
        "    # restores/increases channel dimensions\n",
        "    # final output: 4x more channels than out_channels\n",
        "    self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels * self.expansion)\n",
        "      )\n",
        "\n",
        "    # modifies input instead of creating new tensor\n",
        "    # saves memory during training\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = self.shortcut(x)\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "fcx6kO67glOc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "\n",
        "    # initial convolution modified for CIFAR-10\n",
        "    # see notes for further detail\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # four layers with different number of blocks\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "    # Breaks down to:\n",
        "    # [stride] is [2]\n",
        "    # num_blocks-1 is 3 (4-1)\n",
        "    # [1]*3 creates [1,1,1]\n",
        "    # final result: [2,1,1,1]\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels * block.expansion\n",
        "    # unpacks the list of blocks\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "\n",
        "    out = self.avgpool(out)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "zi02Wl9Zipmr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule:\n",
        "  def __init__(self, batch_size=128, num_workers=2):\n",
        "    self.batch_size = batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "    # define transforms\n",
        "    self.transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    self.transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "  def setup(self):\n",
        "    # download CIFAR-10\n",
        "    self.train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=self.transform_train\n",
        "    )\n",
        "\n",
        "    self.test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=self.transform_test\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=self.num_workers\n",
        "    )\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=self.num_workers\n",
        "    )"
      ],
      "metadata": {
        "id": "ozzHDXCN-jTq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "  def __init__(self, model, data_module, config):\n",
        "    self.model = model\n",
        "    self.data_module = data_module\n",
        "    self.config = config\n",
        "\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = self.model.to(self.device)\n",
        "\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        momentum=config['momentum'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # see notes for why we use CosineAnnealingLR scheduler\n",
        "    self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        self.optimizer,\n",
        "        T_max=config['epochs']\n",
        "    )\n",
        "\n",
        "  def train_epoch(self, epoch):\n",
        "    self.model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # creates a progress bar for training epochs\n",
        "    pbar = tqdm(self.data_module.train_dataloader(), desc=f'Epoch {epoch}')\n",
        "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
        "      inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      outputs = self.model(inputs)\n",
        "      loss = self.criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += targets.size(0)\n",
        "      correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      # update progress bar\n",
        "      pbar.set_postfix({\n",
        "          'loss': train_loss/(batch_idx+1),\n",
        "          'acc': 100.*correct/total\n",
        "      })\n",
        "\n",
        "      # log metrics to wandb\n",
        "      wandb.log({\n",
        "          \"train_loss\": loss.item(),\n",
        "          \"train_acc\": 100.*correct/total,\n",
        "          \"learning_rate\": self.scheduler.get_last_lr()[0]\n",
        "      })\n",
        "\n",
        "  def test_epoch(self):\n",
        "    self.model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in self.data_module.test_dataloader():\n",
        "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    accuracy = 100.*correct/total\n",
        "    avg_loss = test_loss/len(self.data_module.test_dataloader())\n",
        "\n",
        "    # log metrics to wandb\n",
        "    wandb.log({\n",
        "        \"test_loss\": avg_loss,\n",
        "        \"test_acc\": accuracy\n",
        "    })\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def fit(self):\n",
        "    best_acc = 0\n",
        "    for epoch in range(self.config['epochs']):\n",
        "      self.train_epoch(epoch)\n",
        "      accuracy = self.test_epoch()\n",
        "      self.scheduler.step()\n",
        "\n",
        "      # save best model\n",
        "      if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        torch.save(self.model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "id": "l2U_KbYpAclu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see notes for different block configurations\n",
        "\n",
        "def create_resnet18():\n",
        "  return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def create_resnet34():\n",
        "  return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def create_resnet50():\n",
        "  return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def create_resnet101():\n",
        "  return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def create_resnet152():\n",
        "  return ResNet(Bottleneck, [3,8,36,3])"
      ],
      "metadata": {
        "id": "ZqBZgEIaszrY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  'learning_rate': 0.1,\n",
        "  'momentum': 0.9,\n",
        "  'weight_decay': 5e-4,\n",
        "  'epochs': 100,\n",
        "  'batch_size': 128\n",
        "}"
      ],
      "metadata": {
        "id": "vo5WVNvYEhHI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "  project=\"resnet50-cifar10\",\n",
        "  config=config\n",
        ")"
      ],
      "metadata": {
        "id": "A-l5MuCCEzRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and data module\n",
        "model = create_resnet50()\n",
        "data_module = DataModule(batch_size=config['batch_size'])\n",
        "data_module.setup()"
      ],
      "metadata": {
        "id": "7JAqME4HFGz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, data_module, config)\n",
        "trainer.fit()"
      ],
      "metadata": {
        "id": "uiw49eWFFdaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "xi2L4pX3FleN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}