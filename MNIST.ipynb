{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMAdoLfoOGLj6XsGBXu1S9x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Ez8uPjs6iKg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed for reproducibility\n",
        "# ensures the same random numbers are generated every time the script runs\n",
        "torch.manual_seed(42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "6rRswQ8598n8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywctesea_ZQ9",
        "outputId": "83c6ee38-f9e1-4ea9-8e4c-f5eb91ffda1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 496kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.93MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(batch, device):\n",
        "  # move tensor(s) to chose device\n",
        "  if isinstance(batch, (list, tuple)):\n",
        "    return [to_device(x, device) for x in batch]\n",
        "  return batch.to(device)"
      ],
      "metadata": {
        "id": "_gt2Hm-odoif"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeviceDataLoader:\n",
        "  # wrap a dataloader to move data to a device\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    # yield a batch of data after moving it to device\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    # number of batches\n",
        "    return len(self.dl)"
      ],
      "metadata": {
        "id": "oh2BD4BRdFcI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "sgMmiEtPd9Js"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data loaders\n",
        "\n",
        "# batch_size returns 64 images at a time\n",
        "# shuffle randomizes the order of images in each epoch.\n",
        "# prevents the model from learning the order of the data\n",
        "# reduces the risk of getting stuck in local minima\n",
        "# helps the model generalize better\n",
        "train_loader = DeviceDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), device)\n",
        "\n",
        "# use larger batch_size for testing\n",
        "# during testing, you don't perform backpropogation\n",
        "# larger batches are more efficient for evaluation\n",
        "# you want consisted results across the entire test set\n",
        "\n",
        "# no shuffling in for test_data\n",
        "# you're not training on this data, just evaluating\n",
        "# it makes it more reproducible\n",
        "# the order doesn't affect evaluation metrics\n",
        "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=1000, shuffle=False), device)"
      ],
      "metadata": {
        "id": "ujbxSFCRAufk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    # calls the initialization method of the parent class (nn.Module).\n",
        "    # it is necessary because the CNN class inherits from nn.Module base class\n",
        "    super(SimpleCNN, self).__init__()\n",
        "\n",
        "    # convolutional layers\n",
        "\n",
        "    # input color channel = 1 (grayscale)\n",
        "\n",
        "    # OUTPUT CHANNELS = 32. this is a common starting points for\n",
        "    # number of filters in CNNs.\n",
        "    # the number doubles in the next layer - a common practice to\n",
        "    # increase filter count as you go deeper\n",
        "\n",
        "    # KERNEL_SIZE = 3\n",
        "    # 3x3 kernels are the most commonly used in modern CNNs\n",
        "    # generally better than 5x5 or 7x7 because stacking\n",
        "    # multiple 3x3 kernels gives the same receptive field with fewer params\n",
        "\n",
        "    # PADDING=1\n",
        "    # with a 3x3 kernel and padding=1, output size = input size (28x28)\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # INPUT CHANNELS = 32\n",
        "    # matches the output channels from the first layer\n",
        "\n",
        "    # OUTPUT CHANNELS = 64\n",
        "    # doubling # of channels is a common pattern in CNNs\n",
        "    # as you go deeper, you want more feature maps to capture more\n",
        "    # complex patterns.\n",
        "\n",
        "    # same as above\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # pooling layer\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # first convolutional block\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # second convolutional block\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # flatten the output for the fully connected layer\n",
        "    # flattens the 3D representation (channels x height x width) to 1D vector\n",
        "\n",
        "    # -1 automatically calculates this dimension.\n",
        "    # basically means 'however many samples are in the batch\n",
        "\n",
        "    # 64: # of output channels from self.conv2\n",
        "\n",
        "    # 7*7: spatial dimension after two rounds of max pooling:\n",
        "    # MNIST images = 28x28\n",
        "    # first max pooling: 14x14\n",
        "    # second max pooling: 7x7\n",
        "    # reshapes from [32, 64, 7, 7] (batch_size, channels, height, width)\n",
        "    # to [32, 3136] (batch_size, flattened_features). 3136 = 64x7x7\n",
        "    x = x.view(-1, 64 * 7 * 7)\n",
        "\n",
        "    # fully connected layers\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZVdwPCIbCs14"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "FNNHBwCuFkpC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "def train(epochs):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      # forward pass\n",
        "      # data flows through ALL the layers defined in the forward() method\n",
        "      # OUTPUTS: raw model predictions (logits) with shape [batch_size, 10]\n",
        "      outputs = model(images)\n",
        "\n",
        "      # criterion is for classification tasks\n",
        "      # compares predictions to truth\n",
        "      # cross-entropy loss measures the difference between the predicted\n",
        "      # probability distribution and the actual distribution (one-hot encoded labels)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # backward pass and optimize\n",
        "      # CRITICAL: resets the gradients of all parameters (weights and biases) to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # triggers backpropogation algorithm\n",
        "      # after this call, every parameter in the model has a .grad attribute\n",
        "      loss.backward()\n",
        "\n",
        "      # updates all model parameters using the calculated gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate statistics\n",
        "      # accumulates the los value for the current batch into a running total\n",
        "      # useful for calculating average loss over an entire epoch\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # gets the raw tensor data without the computational graph\n",
        "      # torch.max returns 2 values: max value and its index along dim 1 (the class dimension)\n",
        "      # for classification, we care about which class has the highest score, not the actual score value itself\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "      # keeps track of total number of images processed so far\n",
        "      # labels.size(0) gets size of 1st dim of labels tensor, which equals the batch size\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # how many predictions were correct in this batch\n",
        "      # predicted == labels creates boolean tensor that corresponds to the prediction matching the label\n",
        "      # .sum() counts how many True values there are\n",
        "      # .item() converts the tensor to a python scalar\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item(): .4f}')\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader): .4f}, Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "WQTQN9boR2TZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing function\n",
        "def test():\n",
        "  # switch the model to evaluation mode\n",
        "  # dropout layers are disabled (all neurons are active)\n",
        "  # uses running statistics instead of batch statistics\n",
        "  model.eval()\n",
        "  # temporarily disables gradient calculation\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "B79v2EaDTwiH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_lz2Qg8a3rR",
        "outputId": "b466e0e8-5c89-42d6-cb53-f2878118478b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/938], Loss:  0.2348\n",
            "Epoch [1/5], Step [200/938], Loss:  0.2046\n",
            "Epoch [1/5], Step [300/938], Loss:  0.0493\n",
            "Epoch [1/5], Step [400/938], Loss:  0.0282\n",
            "Epoch [1/5], Step [500/938], Loss:  0.0337\n",
            "Epoch [1/5], Step [600/938], Loss:  0.0135\n",
            "Epoch [1/5], Step [700/938], Loss:  0.0356\n",
            "Epoch [1/5], Step [800/938], Loss:  0.0743\n",
            "Epoch [1/5], Step [900/938], Loss:  0.0390\n",
            "Epoch [1/5], Loss:  0.1353, Accuracy: 95.82%\n",
            "Epoch [2/5], Step [100/938], Loss:  0.0220\n",
            "Epoch [2/5], Step [200/938], Loss:  0.0160\n",
            "Epoch [2/5], Step [300/938], Loss:  0.0770\n",
            "Epoch [2/5], Step [400/938], Loss:  0.0212\n",
            "Epoch [2/5], Step [500/938], Loss:  0.0726\n",
            "Epoch [2/5], Step [600/938], Loss:  0.0141\n",
            "Epoch [2/5], Step [700/938], Loss:  0.0702\n",
            "Epoch [2/5], Step [800/938], Loss:  0.0052\n",
            "Epoch [2/5], Step [900/938], Loss:  0.0040\n",
            "Epoch [2/5], Loss:  0.0414, Accuracy: 98.73%\n",
            "Epoch [3/5], Step [100/938], Loss:  0.0181\n",
            "Epoch [3/5], Step [200/938], Loss:  0.0981\n",
            "Epoch [3/5], Step [300/938], Loss:  0.0012\n",
            "Epoch [3/5], Step [400/938], Loss:  0.0251\n",
            "Epoch [3/5], Step [500/938], Loss:  0.0701\n",
            "Epoch [3/5], Step [600/938], Loss:  0.0029\n",
            "Epoch [3/5], Step [700/938], Loss:  0.0634\n",
            "Epoch [3/5], Step [800/938], Loss:  0.0930\n",
            "Epoch [3/5], Step [900/938], Loss:  0.0141\n",
            "Epoch [3/5], Loss:  0.0274, Accuracy: 99.13%\n",
            "Epoch [4/5], Step [100/938], Loss:  0.0135\n",
            "Epoch [4/5], Step [200/938], Loss:  0.0402\n",
            "Epoch [4/5], Step [300/938], Loss:  0.0441\n",
            "Epoch [4/5], Step [400/938], Loss:  0.0020\n",
            "Epoch [4/5], Step [500/938], Loss:  0.0014\n",
            "Epoch [4/5], Step [600/938], Loss:  0.0025\n",
            "Epoch [4/5], Step [700/938], Loss:  0.0271\n",
            "Epoch [4/5], Step [800/938], Loss:  0.0110\n",
            "Epoch [4/5], Step [900/938], Loss:  0.0359\n",
            "Epoch [4/5], Loss:  0.0216, Accuracy: 99.26%\n",
            "Epoch [5/5], Step [100/938], Loss:  0.0023\n",
            "Epoch [5/5], Step [200/938], Loss:  0.0112\n",
            "Epoch [5/5], Step [300/938], Loss:  0.0306\n",
            "Epoch [5/5], Step [400/938], Loss:  0.0398\n",
            "Epoch [5/5], Step [500/938], Loss:  0.0019\n",
            "Epoch [5/5], Step [600/938], Loss:  0.0050\n",
            "Epoch [5/5], Step [700/938], Loss:  0.0182\n",
            "Epoch [5/5], Step [800/938], Loss:  0.0023\n",
            "Epoch [5/5], Step [900/938], Loss:  0.0058\n",
            "Epoch [5/5], Loss:  0.0148, Accuracy: 99.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1DczfLEbjvJ",
        "outputId": "cc388110-38a0-46e5-e648-24de492a01fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1h1kWCOfE24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}