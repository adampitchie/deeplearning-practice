{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIYHjuZUxcDSx/1SFNptcH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q"
      ],
      "metadata": {
        "id": "u5oVAs-Npnyq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb"
      ],
      "metadata": {
        "id": "OaQarNgwpbrJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "gasXx83eqKNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"mnist-cnn\",\n",
        "    config={\n",
        "        \"architecture\": \"SimpleCNN\",\n",
        "        \"dataset\": \"MNIST\",\n",
        "        \"epochs\": 5,\n",
        "        \"batch_size\": 64,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "LZG81pO1rOfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed for reproducibility\n",
        "# ensures the same random numbers are generated every time the script runs\n",
        "torch.manual_seed(42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "6rRswQ8598n8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "ywctesea_ZQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(batch, device):\n",
        "  # move tensor(s) to chose device\n",
        "  if isinstance(batch, (list, tuple)):\n",
        "    return [to_device(x, device) for x in batch]\n",
        "  return batch.to(device)"
      ],
      "metadata": {
        "id": "_gt2Hm-odoif"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeviceDataLoader:\n",
        "  # wrap a dataloader to move data to a device\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    # yield a batch of data after moving it to device\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    # number of batches\n",
        "    return len(self.dl)"
      ],
      "metadata": {
        "id": "oh2BD4BRdFcI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "sgMmiEtPd9Js"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data loaders\n",
        "\n",
        "# batch_size returns 64 images at a time\n",
        "# shuffle randomizes the order of images in each epoch.\n",
        "# prevents the model from learning the order of the data\n",
        "# reduces the risk of getting stuck in local minima\n",
        "# helps the model generalize better\n",
        "train_loader = DeviceDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), device)\n",
        "\n",
        "# use larger batch_size for testing\n",
        "# during testing, you don't perform backpropogation\n",
        "# larger batches are more efficient for evaluation\n",
        "# you want consisted results across the entire test set\n",
        "\n",
        "# no shuffling in for test_data\n",
        "# you're not training on this data, just evaluating\n",
        "# it makes it more reproducible\n",
        "# the order doesn't affect evaluation metrics\n",
        "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=1000, shuffle=False), device)"
      ],
      "metadata": {
        "id": "ujbxSFCRAufk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    # calls the initialization method of the parent class (nn.Module).\n",
        "    # it is necessary because the CNN class inherits from nn.Module base class\n",
        "    super(SimpleCNN, self).__init__()\n",
        "\n",
        "    # convolutional layers\n",
        "\n",
        "    # input color channel = 1 (grayscale)\n",
        "\n",
        "    # OUTPUT CHANNELS = 32. this is a common starting points for\n",
        "    # number of filters in CNNs.\n",
        "    # the number doubles in the next layer - a common practice to\n",
        "    # increase filter count as you go deeper\n",
        "\n",
        "    # KERNEL_SIZE = 3\n",
        "    # 3x3 kernels are the most commonly used in modern CNNs\n",
        "    # generally better than 5x5 or 7x7 because stacking\n",
        "    # multiple 3x3 kernels gives the same receptive field with fewer params\n",
        "\n",
        "    # PADDING=1\n",
        "    # with a 3x3 kernel and padding=1, output size = input size (28x28)\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # INPUT CHANNELS = 32\n",
        "    # matches the output channels from the first layer\n",
        "\n",
        "    # OUTPUT CHANNELS = 64\n",
        "    # doubling # of channels is a common pattern in CNNs\n",
        "    # as you go deeper, you want more feature maps to capture more\n",
        "    # complex patterns.\n",
        "\n",
        "    # same as above\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # pooling layer\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # first convolutional block\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # second convolutional block\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # flatten the output for the fully connected layer\n",
        "    # flattens the 3D representation (channels x height x width) to 1D vector\n",
        "\n",
        "    # -1 automatically calculates this dimension.\n",
        "    # basically means 'however many samples are in the batch\n",
        "\n",
        "    # 64: # of output channels from self.conv2\n",
        "\n",
        "    # 7*7: spatial dimension after two rounds of max pooling:\n",
        "    # MNIST images = 28x28\n",
        "    # first max pooling: 14x14\n",
        "    # second max pooling: 7x7\n",
        "    # reshapes from [32, 64, 7, 7] (batch_size, channels, height, width)\n",
        "    # to [32, 3136] (batch_size, flattened_features). 3136 = 64x7x7\n",
        "    x = x.view(-1, 64 * 7 * 7)\n",
        "\n",
        "    # fully connected layers\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZVdwPCIbCs14"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# log gradients and model parameters\n",
        "wandb.watch(model, log=\"all\")"
      ],
      "metadata": {
        "id": "FNNHBwCuFkpC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "def train(epochs):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      # forward pass\n",
        "      # data flows through ALL the layers defined in the forward() method\n",
        "      # OUTPUTS: raw model predictions (logits) with shape [batch_size, 10]\n",
        "      outputs = model(images)\n",
        "\n",
        "      # criterion is for classification tasks\n",
        "      # compares predictions to truth\n",
        "      # cross-entropy loss measures the difference between the predicted\n",
        "      # probability distribution and the actual distribution (one-hot encoded labels)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # backward pass and optimize\n",
        "      # CRITICAL: resets the gradients of all parameters (weights and biases) to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # triggers backpropogation algorithm\n",
        "      # after this call, every parameter in the model has a .grad attribute\n",
        "      loss.backward()\n",
        "\n",
        "      # updates all model parameters using the calculated gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate statistics\n",
        "      # accumulates the los value for the current batch into a running total\n",
        "      # useful for calculating average loss over an entire epoch\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # gets the raw tensor data without the computational graph\n",
        "      # torch.max returns 2 values: max value and its index along dim 1 (the class dimension)\n",
        "      # for classification, we care about which class has the highest score, not the actual score value itself\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "      # keeps track of total number of images processed so far\n",
        "      # labels.size(0) gets size of 1st dim of labels tensor, which equals the batch size\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # how many predictions were correct in this batch\n",
        "      # predicted == labels creates boolean tensor that corresponds to the prediction matching the label\n",
        "      # .sum() counts how many True values there are\n",
        "      # .item() converts the tensor to a python scalar\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      # log batch statistcs\n",
        "      if (i + 1) % 100 == 0:\n",
        "        batch_accuracy = 100 * correct / total\n",
        "        avg_loss = running_loss / (i + 1)\n",
        "\n",
        "        wandb.log({\n",
        "            \"batch\": i + 1 + epoch * len(train_loader),\n",
        "            \"batch_loss\": loss.item(),\n",
        "            \"batch_accuracy\": batch_accuracy,\n",
        "            \"running_loss\": avg_loss\n",
        "        })\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item(): .4f}, Accuracy: {batch_accuracy:.2f}%')\n",
        "\n",
        "    #  calculate epoch statistics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    # log epoch statistics\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"epoch_loss\": epoch_loss,\n",
        "        \"epoch_accuracy\": epoch_accuracy\n",
        "    })\n",
        "\n",
        "    test()"
      ],
      "metadata": {
        "id": "WQTQN9boR2TZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing function\n",
        "def test():\n",
        "  # switch the model to evaluation mode\n",
        "  # dropout layers are disabled (all neurons are active)\n",
        "  # uses running statistics instead of batch statistics\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "\n",
        "  class_correct = [0] * 10\n",
        "  class_total = [0] * 10\n",
        "\n",
        "  # temporarily disables gradient calculation\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # calculate overall metrics\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    wandb.log({\n",
        "        \"test_loss\": avg_test_loss,\n",
        "        \"test_accuracy\": test_accuracy,\n",
        "    })\n",
        "\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "B79v2EaDTwiH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_lz2Qg8a3rR",
        "outputId": "231d942b-eae9-4e49-9fdf-0007186f82fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/938], Loss:  0.0177, Accuracy: 98.62%\n",
            "Epoch [1/5], Step [200/938], Loss:  0.0183, Accuracy: 98.67%\n",
            "Epoch [1/5], Step [300/938], Loss:  0.0339, Accuracy: 98.68%\n",
            "Epoch [1/5], Step [400/938], Loss:  0.0202, Accuracy: 98.69%\n",
            "Epoch [1/5], Step [500/938], Loss:  0.0168, Accuracy: 98.74%\n",
            "Epoch [1/5], Step [600/938], Loss:  0.0024, Accuracy: 98.73%\n",
            "Epoch [1/5], Step [700/938], Loss:  0.0059, Accuracy: 98.74%\n",
            "Epoch [1/5], Step [800/938], Loss:  0.0191, Accuracy: 98.73%\n",
            "Epoch [1/5], Step [900/938], Loss:  0.0351, Accuracy: 98.75%\n",
            "Test Accuracy: 98.66%\n",
            "Epoch [2/5], Step [100/938], Loss:  0.0608, Accuracy: 99.00%\n",
            "Epoch [2/5], Step [200/938], Loss:  0.0392, Accuracy: 99.05%\n",
            "Epoch [2/5], Step [300/938], Loss:  0.1138, Accuracy: 99.04%\n",
            "Epoch [2/5], Step [400/938], Loss:  0.0078, Accuracy: 99.06%\n",
            "Epoch [2/5], Step [500/938], Loss:  0.0016, Accuracy: 99.05%\n",
            "Epoch [2/5], Step [600/938], Loss:  0.0028, Accuracy: 99.08%\n",
            "Epoch [2/5], Step [700/938], Loss:  0.0378, Accuracy: 99.10%\n",
            "Epoch [2/5], Step [800/938], Loss:  0.0112, Accuracy: 99.09%\n",
            "Epoch [2/5], Step [900/938], Loss:  0.0411, Accuracy: 99.09%\n",
            "Test Accuracy: 98.97%\n",
            "Epoch [3/5], Step [100/938], Loss:  0.0119, Accuracy: 99.50%\n",
            "Epoch [3/5], Step [200/938], Loss:  0.0057, Accuracy: 99.55%\n",
            "Epoch [3/5], Step [300/938], Loss:  0.0165, Accuracy: 99.49%\n",
            "Epoch [3/5], Step [400/938], Loss:  0.0787, Accuracy: 99.44%\n",
            "Epoch [3/5], Step [500/938], Loss:  0.0071, Accuracy: 99.43%\n",
            "Epoch [3/5], Step [600/938], Loss:  0.0302, Accuracy: 99.44%\n",
            "Epoch [3/5], Step [700/938], Loss:  0.0385, Accuracy: 99.43%\n",
            "Epoch [3/5], Step [800/938], Loss:  0.0612, Accuracy: 99.39%\n",
            "Epoch [3/5], Step [900/938], Loss:  0.0086, Accuracy: 99.38%\n",
            "Test Accuracy: 98.83%\n",
            "Epoch [4/5], Step [100/938], Loss:  0.0006, Accuracy: 99.70%\n",
            "Epoch [4/5], Step [200/938], Loss:  0.0215, Accuracy: 99.62%\n",
            "Epoch [4/5], Step [300/938], Loss:  0.0021, Accuracy: 99.61%\n",
            "Epoch [4/5], Step [400/938], Loss:  0.0528, Accuracy: 99.61%\n",
            "Epoch [4/5], Step [500/938], Loss:  0.0039, Accuracy: 99.57%\n",
            "Epoch [4/5], Step [600/938], Loss:  0.0006, Accuracy: 99.55%\n",
            "Epoch [4/5], Step [700/938], Loss:  0.0618, Accuracy: 99.54%\n",
            "Epoch [4/5], Step [800/938], Loss:  0.0013, Accuracy: 99.52%\n",
            "Epoch [4/5], Step [900/938], Loss:  0.0033, Accuracy: 99.53%\n",
            "Test Accuracy: 99.05%\n",
            "Epoch [5/5], Step [100/938], Loss:  0.0023, Accuracy: 99.69%\n",
            "Epoch [5/5], Step [200/938], Loss:  0.0202, Accuracy: 99.69%\n",
            "Epoch [5/5], Step [300/938], Loss:  0.0033, Accuracy: 99.66%\n",
            "Epoch [5/5], Step [400/938], Loss:  0.0020, Accuracy: 99.64%\n",
            "Epoch [5/5], Step [500/938], Loss:  0.0015, Accuracy: 99.62%\n",
            "Epoch [5/5], Step [600/938], Loss:  0.0052, Accuracy: 99.64%\n",
            "Epoch [5/5], Step [700/938], Loss:  0.0014, Accuracy: 99.63%\n",
            "Epoch [5/5], Step [800/938], Loss:  0.0036, Accuracy: 99.62%\n",
            "Epoch [5/5], Step [900/938], Loss:  0.2278, Accuracy: 99.61%\n",
            "Test Accuracy: 98.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NC4D53KezTFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}